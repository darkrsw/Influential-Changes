\section{Related Work}
\label{sec:related}

This section discusses four groups of related work; 1) software evolution, 2)
change impact analysis, 3) defect prediction, and 4) developer expertise. These
topics address several relevant aspects of our study.

\subsection{Software Evolution}
Changing any file in a software system implies that the system evolves in a
certain direction. Many studies dealt with software evolution in different ways.
D'Ambros et al.~\cite{dambros_evolution_2006} presented \emph{the evolution
radar} that visualizes file and module-level coupling information. Although this
tool does not directly predict or analyze the change impact, it can show an
overview of coupling relationships between files and modules.
Chronos~\cite{servant_history_2012} provides a narrowed view of history
slicing for a specific file. The tool analyzes a line-level history of a file.
This reduces the time required to resolve program evolution tasks. Girba et
al.~\cite{girba_how_2005} proposed a metric called \emph{code ownership} to
illustrate how developers drive software evolution. We used the metric to
examine the influence of a change.

API evolution and deprecation, which often leads to important collateral evolutions
can be influential in project development. Several studies~\cite{Dagenais:2009:SAR:1555001.1555083,
robbes_how_2012,sawant_reaction_2016,Teyton:2014:SLM:2926316.2926322,Kim:2010:MAI:1831508.1831654,Dagenais:2011:RAC:2000799.2000805,6062100} in the literature have
investigated such evolution. For example, Dagenais and Robillard have proposed SemDiff~\cite{Dagenais:2009:SAR:1555001.1555083},
a framework for recommending replacements of non-trivial changes. Robbes et al.~\cite{robbes_how_2012}
have studied the importance in practice of API deprecation for developers. Teyton et al.~\cite{Teyton:2014:SLM:2926316.2926322} have built a framework for analyzing software library dependencies to support developers in library migrations based on evolution trends.



\subsection{Change Impact Analysis}


Many previous studies revealed a potential impact of software changes. There is a
set of techniques that use dynamic analysis to identify change impacts. Ren et
al.~\cite{ren_chianti:_2004} proposed \emph{Chianti}. This tool first runs test
cases on two subsequent program revisions (after/before a change) to figure out
atomic changes that describe behavioral differences. The authors provided a
plug-in for Eclipse, which helps developers browse a change impact set of a
certain atomic change. FaultTracer~\cite{zhang_faulttracer:_2012} identifies
a change impact set by differentiating the results of test case executions on
two different revisions. This tool uses the extended call graphs to select
test cases affected by a change.

Brudaru and Zeller~\cite{brudaru_what_2008} pointed out that the long-term
impact of changes must be identified. To deal with the long-term impact, the authors proposed
a change genealogy graph, which keeps track of dependencies between
subsequent changes. Change genealogy captures addition/change/ deletion of
methods in a program. It can measure the long-term impact on quality,
maintainability, and stability~\cite{herzig_capturing_2010}. 
In addition, it can reveal cause-effect chains~\cite{herzig_mining_2011} and predict
defects~\cite{herzig_predicting_2013}.


Although dynamic analysis and change genealogy can pinpoint a specific element
affected by a change in source code, its scope is limited to executed statements
by test cases. This can miss many affected elements in source code as well as
non-source code files such as build scripts and configuration settings.
Revision histories can be used for figuring out files changed frequently
together. Zimmermann et al.~\cite{zimmermann_mining_2004} first studied
co-change analysis in which the authors revealed that some files are
commonly changed together. Ying et al.~\cite{ying_predicting_2004} proposed an
approach to predicting files to change together based on revision histories.


There have been cluster-based techniques for change impact analysis. Robillard
and Dagenais~\cite{robillard_retrieving_2008} proposed an approach to building
change clusters based on revision histories. Clusters are retrieved by analyzing
program elements commonly changed together in change sets. Then, the approach
attempts to find matching clusters for a given change. The matching clusters are
regarded as the change impact of the given change. Sherriff and
Williams~\cite{sherriff_empirical_2008} presented a technique for change impact
analysis using singular value decomposition (SVD). This technique basically
figures out clusters of program elements frequently changed together. When clustering
changes, the technique performs SVD. The clusters can be used for identifying
the change impact of an incoming change. 
 
% \dongsun{we need to discuss~\cite{shihab_industrial_2012} and~\cite{shihab_high-impact_2011}.}

In addition, Shihab et al. studied risky changes~\cite{shihab_industrial_2012} in an industrial project. Risky changes are, rather than just a bug, software changes that developers need to pay additional attention to avoid a negative impact on their product. They built a prediction model based on change-relevant features such as change time, code size, and the number of files changed. The model achieves 67\% recall and 87\% precision. The study explored a different perspective of influential changes; the context was an industrial case, and risky changes are labeled by the developers. On the other hand, our study revealed latent ICs in open source projects. Combining these two perspectives is our future direction.

Identifying high-impact defects~\cite{shihab_high-impact_2011} is highly-relevant to our work as well. This type of defects includes breakage and surprise defects in detail. Shihab~\cite{shihab_high-impact_2011} suggested a prediction model to classify the high-impact defect types. They figured out that breakage defects are relevant to the number of pre-release defects and file size while the time between the latest pre-release change and the release date lead to surprise defects. These findings are helpful to find latent ICs in addition to our methods.


%AOP-based. Necessary here?
% Liu et al.~\cite{liu_change_2011} presented an approach to identify change
% impacts in aspect-oriented programs. This approach analyzes
% potential control-flow changes to search for program dependencies when the
% program is woven by aspects.

\subsection{Change Pattern Mining}
Discovering common change patterns may reveal influential changes since those patterns
appear across different projects.
% Common change patterns are useful for various purposes. 
There have been several empirical studies on change patterns.
Pan et al.~\cite{pan2009toward} explored common bug fix patterns in Java programs to understand how developers change programs to fix a bug. 
Their fix patterns are in a high-level schema (e.g. ``If-related: Addition of Post-condition Check (IF-APTC)'').
Based on the insight, PAR~\cite{kim2013automatic} leveraged common pre-defined fix patterns for automated program repair, that only contain six fix patterns which can only be used to fix a small number of bugs. 
Martinez and Monperrus further investigated repair models that can be utilized in program fixing while Zhong and Su~\cite{zhong2015empirical} conducted a large-scale study on bug fixing changes in open source projects. Tan et al.~\cite{tan2016anti} analyzed anti-patterns that may interfere with the process of automated program repair. 
While most pattern discovery studies focused on statement-level (i.e., coarse-grained) change patterns, 
Liu et al.~\cite{liu_closer_2018} investigated fine-grained (e.g., expression-level) code changes to
discover better change elements for progra repair.
Koyuncu et al.~\cite{koyuncu_impact_2017} explored how developers manually or automatically make program changes in
the Linux kernel project and their impact; this study showed changes generated by different tools may have 
different impacts on a software project.
% However, all of them studied code changes at the statement level, which is not as fine-grained as our work that extracts fine-grained code changes with an extended version of GumTree~\cite{falleri2014fine}.


% \paragraph*{\it Pattern Mining for Code Change} 
In addition, several studies have proposed techniques to automate change pattern discovery.
SYDIT~\cite{meng2011systematic} and Lase~\cite{meng2013lase} generate code changes to other code snippets with the extracted edit scripts from examples in the same application. 
RASE~\cite{meng2015does} focuses on refactoring code clones with Lase edit scripts~\cite{meng2013lase}.  
FixMeUp~\cite{son2011rolecast} extracts and applies access control templates to protect sensitive operations. Their objectives are not to address issues caused by faulty code in program, such as the static analysis bugs studied in this study. 
REFAZER implements an algorithm for learning syntactic program transformations for C\# programs from examples~\cite{rolim2017learning} to correct defects in student submissions, which however are mostly useless across assignments~\cite{long2017automatic} and are not really defects in the wild as the violations in our study. Genesis~\cite{long2017automatic} heuristically infers application-independent code transform patterns from multiple applications to fix bugs, but its code transform patterns are tightly coupled with the nature and syntax of three kinds of bugs (i.e., null pointer, out of bounds, and class cast defects). 
Koyuncu et al.~\cite{koyuncu2018fixminer} have generalized this approach with FixMiner to mining fix patterns for all types of bugs given a large dataset. 
% Our work tries to mine the common fix patterns for general static analysis violations which are not application-independent. 
% Closely related to our work is the concurrent work of 
Reudismam et al.~\cite{rolim2018learning} tried to learn quick fixes by mining code changes to fix PMD violations~\cite{pmd}. Their approach aims at learning code change templates to be systematically applied to refactor code. 
Li et al.~\cite{liu_mining_2018} leveraged convolutional neural networks, which is one of the deep neural network techniques,
to automatically cluster commons fix patterns from more than 88,000 bug-fixing changes. Those fix patterns
can successfully fix real bugs~\cite{liu_avatar_2019}.

% Our approach can be used for a similar scenario, and scales to a huge variety of violation types.

\subsection{Program repair}

Automated program repair (APR) can promote influential changes since most
program repair techniques focus on common and recurring bug types.
These techniques often locate and fix similar bugs by scanning all files in a project.
Thus, applying APR techniques can produce massive program changes at once
and this could be influential for the further evolution of the project.



% This paragraph can be omitted.
% Recent studies of program repair have presented several achievements. There are mainly two lines of research: (1) fully automated repair and (2) patch hint suggestion. The former focuses on automatically generating patches that can be integrated into a program without human intervention. The patch generation process often includes patch verification to figure out whether the patch does not break the original functionality when it is applied to the program. The verification is often achieved by running a given test suite. Automatize violation repair is included in our future work. The latter techniques suggest code fragments that can help create a patch rather than generating a patch ready to integrate. Developers may use the suggestions to write patches and verify them manually, that is similar to the patch generation of our work.

% \paragraph*{\it Fully Automated Repair} 
Most of program repair studies focus on automating the entire process of fixing bugs, i.e., localizaing a bug, 
generating a patch, validating the patch.
Automated program repair is pioneered by GenProg~\cite{weimer2009automatically,le2012genprog}. This approach leverages genetic programming to create a patch for a given buggy program. It is followed by an acceptability study~\cite{fry2012human} and systematic evaluation~\cite{le2012systematic}. Regarding the acceptability issue, Kim et al.~\cite{kim2013automatic} advocated GenProg may generate nonsensical patches and proposed PAR to deal with the issue. PAR leverages human-written patches to define fix templates and can generate more acceptable patches. 
HDRepair~\cite{le2016history} leverages bug fixing history of many projects to provide better patch candidates to the random search process.
Recently, LSRrepair~\cite{kui2018live} proposes a live search approach to the ingredients of automated repair using code search techniques. 
While GenProg relies on randomness, utilizing program synthesis techniques~\cite{nguyen2013semfix,mechtaev2016angelix,mechtaev2015directfix} can directly generate patches even though they are limited to a certain subset of bugs.
Other notable approaches include contract-based fixing~\cite{wei2010automated}, program repair based on behavior models~\cite{dallmeier2009generating}, and conditional statement repair~\cite{xuan2017nopol}. 
% This study does not focus on the fully automated program repair but the automated fix pattern mining for violations.

% \paragraph*{\it Patch Hint Suggestion} 
Patch suggestion studies explored diverse dimensions. MintHint~\cite{kaleeswaran2014minthint} generates repair hints based on statistical analysis. Tao et al.~\cite{tao2014automatically} investigated how automatically generated patches can be used as debugging aids. 
Bissyand{\'e} suggests patches for bug reports based on the history of patches~\cite{bissyande2015harvesting}. 
Caramel~\cite{nistor2015caramel} focuses on potential performance defects and suggests specific types of patches to fix those defects. Our study is closely related to patch hint suggestion since we can suggest top-10 most similar fix patterns for targeting violations. 
% The difference is that fix patterns in this work are mined from developers' patch submissions of static analysis violations.

% \paragraph*{\it Empirical Studies on Program Repair} 
Many studies have explored properties of program repair. Monperrus~\cite{monperrus2014critical} criticized issues of patch generation learned from human-written patches~\cite{kim2013automatic}. Barr et al. discussed the plastic surgery hypothesis~\cite{barr2014plastic} that theoretically illustrates graftibility of bugs from a given program.
Long and Rinard analyzed the search space issues for population-based patch generation~\cite{long2016analysis}. Smith et al. presented an argument of overfitting issues of program repair techniques~\cite{smith2015cure}. Koyuncu et al.~\cite{koyuncu2017impact} compared the impact of different patch generation techniques in Linux kernel development.
Benchmarks for program repair are proposed for different programming languages~\cite{le2015manybugs, just2014defects4j}. Based on a benchmark, a large-scale replication study was conducted~\cite{martinez2017automatic}.
% More recently, Liu et al.~\cite{kui2018closer} investigated the distribution of code entities impacted by bug fixes with fine-grained granularity, and found that some static analysis tools (e.g., FindBugs~\cite{findbugs:description} and PMD~\cite{pmd}) are involved in some bug fixes.



\subsection{Defect Prediction}

Changing a program may often introduce
faults~\cite{sliwerski_hatari:_2005,kim_automatic_2006}.
Thus, fault prediction at an early stage can lead developers to achieve
a better software quality. Kim et al.~\cite{kim_predicting_2007} proposed a
cache-based model to predict whether an incoming change may introduce or not.
They used \emph{BugCache} and \emph{FixCache} that record entities and files
likely to introduce a bug and fix the bug if they are changed. The results of
their empirical study showed that the caches 46-95\% accuracy in seven open
source projects.

Machine learning classification can be used for defect prediction as well. Kim
et al.~\cite{kim_classifying_2008} presented an approach to classifying
software changes into buggy or clean ones. They used several features such as number of
lines of added/deleted code, terms in change logs, and cyclomatic complexity.
The authors conducted an empirical evaluation on 12 open source projects. The
result shows 78\% prediction accuracy on average. In addition, Shivaji et
al.~\cite{shivaji_reducing_2009} proposed a feature selection technique to
improve the prediction performance of defect prediction. Features are not
limited to metrics of source code; Jiang et al.~\cite{jiang_personalized_2013}
built a prediction model based on individual developers. Defect prediction
techniques are often faced with imbalanced datasets. Bird et
al.~\cite{bird_fair_2009} pointed out that unfair and imbalanced datasets can
lead to bias in defect prediction.


\subsection{Developer Expertise}

It is necessary to discuss developer expertise since influential changes imply
that the developer who made the changes can be influential to other developers.

As the size of open-source software projects is getting larger, developer
networks are naturally constructed, and every activity in the network may affect
other developers substantially. Hong et al.~\cite{hong_understanding_2011}
reported a result of observing a developer social network. The authors
investigated Mozilla's bug tracking site to construct a developer social network
(DSN). In addition, they collected general social networks (GSNs) from ordinary
social communities such as Facebook and Amazon. This paper provides a
comparison between DSN and GSNs. Findings described in this paper include 1) DSN
does not follow power-law degree distribution while GSNs do, 2) the size of
communities in DSNs is smaller than that of GSNs. This paper also reports the
result of an evolution analysis on DSNs. DSNs tend to grow over time but not much as
GSNs do.

Onoue et al.~\cite{onoue_study_2013} studied and enumerated developer activity
data in \texttt{Github.com}.
It classifies good developers, tries to understand developers, and
differentiates types of developers. However, the paper does not provide any
further implication. In addition, there is no result of role analysis and 
social structure.

Pham et al.~\cite{pham_creating_2013} reported the results of a user study which
has been conducted to reveal a testing culture in OSS. The authors have
interviewed 33 developers of GitHub first and figured out the transparency of
testing behaviors. Then, an online questionnaire has been sent to 569 developers
of GitHub to find out testing strategies.
