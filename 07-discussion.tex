\section{Limitations \& Threats to Validity}
\label{sec:discussion}

The results of our study support the concept of ``risky changes'' introduced by Shihab et al.~\cite{shihab_industrial_2012}.
Influential changes can be risky (in the sense that they may have negative effects) but may also be stimulating (in the sense that
they may have positive impacts). We have further considered in our work, that the ``influence'' of the changes can be
visible on three entities: the code base evolution, the user base adoption, and the developer team dynamics.

Concretely, the categories that we have inferred and validated with code reviewers, as well as the dataset that
we have built are important artefacts that can spark research around the topic of influential changes.

Our work, which we recognize as a preliminary investigation, opens new research directions for
recommending changes to the attention of other developers in the team, for dismissing or delaying changes that may
affect software adoption, for rewarding/penalizing authors of changes that will have positive/negative effects.

The main limitations of our work include:
\begin{itemize}
	\item the generality in defining and identifying influential changes. We have opted in this work to not focus on specific category of influential changes. Instead, we investigate them broadly and leave in-depth characterization to future works.
	\item our classification model, which shows high performance in predicting influential changes, could be entirely overfitting to  the dataset. Future work would require application of the model to a larger and more diverse dataset.
\end{itemize}

Our study raises several threats to validity. This section outlines the most salient ones.

\textbf{Internal validity.} The authors have manually labelled themselves the influential changes as it
was prohibitively costly to request labelling by a large number of developers. We have mitigated this issue
by clearly defining criteria for selecting influential changes, and by performing cross-checking.
Another threat relates to the number of developers who participated to the code developer study for
approving the categories of influential changes. We have attempted to mitigate this threat by launching
advertisement campaigns targeting thousands of developers. We have further focused on quality and representative 
developers by targeting those with some code review experience.

% The manual labelling of influential changes In the collection of datasets for the
%observational study, 
%we took steps to curate the data to remove noise. For example, we dismissed
%Git merge commits and Git cherry pick commits. In addition, we conducted a
%manual analysis to identify influential changes in the post-mortem observational
%study. The manual analysis might be subjective but we tried to mitigate this
%issue by having regular discussion sessions. In the sessions, the authors
%evaluated and analyzed the changes to figure out their influence clearly.


\textbf{External validity.} Although we considered a large dataset of commit
changes, this data may not represent the universe of real-world programs.
Indeed, the study focused on open-source software projects written in Java.
The metrics and features used for predicting influential changes in this context
may not be representative for other contexts.


\textbf{Construct validity.} Finally, we selected features based on
our intuitions on influential changes. Our study may have thus overlooked more
discriminative features. To mitigate this threat, first we have considered several
features, many of which are commonly known in the literature, second we have repeated
the experiments based on data labelled following  new category labels of influential changes
approved by developers. 

Furthermore, our experiment focuses only on project-specific classification rather than cross-project setting. Cross-project classification can be useful since some projects may not have sufficient number of data (i.e., commits) to build a training set. However, NL features are project-specific and cannot be
applied to cross-project setting. 
Our future work includes extracting more features for cross-project IC classification.
